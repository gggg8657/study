{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msn\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m  \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSegmentedColormap\n\u001b[1;32m     14\u001b[0m W \u001b[38;5;241m=\u001b[39m LinearSegmentedColormap\u001b[38;5;241m.\u001b[39mfrom_list(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m], N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This gist is an extract from:\n",
    "https://github.com/epignatelli/reinforcement-learning-an-introduction\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "W = LinearSegmentedColormap.from_list('w',[\"w\", \"w\"], N=256)\n",
    "\n",
    "ACTIONS = {\n",
    "    0: [1, 0],   # north\n",
    "    1: [-1, 0],  # south\n",
    "    2: [0, -1],  # west\n",
    "    3: [0, 1],   # east\n",
    "}\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        \"\"\"\n",
    "        A gridworld environment with absorbing states at [0, 0] and [size - 1, size - 1].\n",
    "        Args:\n",
    "            size (int): the dimension of the grid in each direction\n",
    "            cell_reward (float): the reward return after extiting any non absorbing state\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.state_value = np.zeros((size, size))\n",
    "        return\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # is terminal state?\n",
    "        size = len(self.state_value) - 1\n",
    "        if (state == (0, 0)) or (state == (size, size)):\n",
    "            return state, 0\n",
    "\n",
    "        s_1 = (state[0] + action[0], state[1] + action[1])\n",
    "        reward = -1\n",
    "        # out of bounds north-south\n",
    "        if s_1[0] < 0 or s_1[0] >= len(self.state_value):\n",
    "            s_1 = state\n",
    "        # out of bounds east-west\n",
    "        elif s_1[1] < 0 or s_1[1] >= len(self.state_value):\n",
    "            s_1 = state\n",
    "\n",
    "        return s_1, reward\n",
    "\n",
    "    def render(self, title=None):\n",
    "        \"\"\"\n",
    "        Displays the current value table of mini gridworld environment\n",
    "        \"\"\"\n",
    "        size = len(self.state_value) if len(self.state_value) < 20 else 20\n",
    "        fig, ax = plt.subplots(figsize=(size, size))\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "        return sn.heatmap(self.state_value, annot=True, fmt=\".1f\", cmap=W, linewidths=1, linecolor=\"black\", cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1779648520.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [3], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    def bellman_expectation(self, state, probs, discount):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This gist is an extract from:\n",
    "https://github.com/epignatelli/reinforcement-learning-an-introduction\n",
    "\"\"\"\n",
    "\n",
    "    def bellman_expectation(self, state, probs, discount):\n",
    "        \"\"\"\n",
    "        Makes a one step lookahead and applies the bellman expectation equation to the state self.state_value[state]\n",
    "        Args:\n",
    "            state (Tuple[int, int]): the x, y indices that define the address on the value table\n",
    "            probs (List[float]): transition probabilities for each action\n",
    "            in_place (bool): if False, the value table is updated after all the new values have been calculated.\n",
    "                             if True the state [i, j] will new already new values for the states [< i, < j]\n",
    "        Returns:\n",
    "            (numpy.ndarrray): the new value for the specified state\n",
    "        \"\"\"\n",
    "        # absorbing state\n",
    "        value = 0\n",
    "        for c, action in ACTIONS.items():\n",
    "            s_1, reward = self.step(state, action)\n",
    "            value += probs[c] * (reward + discount * self.state_value[s_1])\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This gist is an extract from:\n",
    "https://github.com/epignatelli/reinforcement-learning-an-introduction\n",
    "\"\"\"\n",
    "\n",
    "def policy_evaluation(env, policy=None, steps=1, discount=1., in_place=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy (numpy.array): a numpy 3-D numpy array, where the first two dimensions identify a state and the third dimension identifies the actions.\n",
    "                              The array stores the probability of taking each action.\n",
    "        steps (int): the number of iterations of the algorithm\n",
    "        discount (float): discount factor for the bellman equations\n",
    "        in_place (bool): if False, the value table is updated after all the new values have been calculated.\n",
    "             if True the state [i, j] will new already new values for the states [< i, < j]\n",
    "    \"\"\"\n",
    "    if policy is None:\n",
    "        # uniform random policy\n",
    "        policy = np.ones((*env.state_value.shape, len(ACTIONS))) * 0.25\n",
    "\n",
    "\n",
    "    for k in range(steps):\n",
    "        # cache old values if not in place\n",
    "        values = env.state_value if in_place else np.empty_like(env.state_value)\n",
    "        for i in range(len(env.state_value)):\n",
    "            for j in range(len(env.state_value[i])):\n",
    "                # apply bellman expectation equation to each state\n",
    "                state = (i, j)\n",
    "                value = env.bellman_expectation(state, policy[i, j], discount)\n",
    "                values[i, j] = value * discount\n",
    "        # set the new value table\n",
    "        env.state_value = values\n",
    "    return env.state_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
