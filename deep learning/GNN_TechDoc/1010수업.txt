relu 전에 normalization 안쓰는 이유
	: relu 를 왜 쓰게 됐지? Vanishing gradient 문제를 해결하기 위해 사용
	: sigmoid만 있으면 모든 함수에 대한 근사화 가능
	: error 도 0이 되니까, gradient가 아래계층으로 갈수록 학습이 잘 안될것이다.
	: 양수에서는 무한대로 간다.
	: 그냥 단순히 수학적으로 normalize 가 뭔지를 생각해라. 값에 대한 shift 
	: linear layer 에서 비슷한 과정 하는데 왜 또 같은 과정을 반복하냐
	: 다 간과하고 지나갔던 것들이다.
	: 모델들이 연구될때는 이런게 다 감안이 됐을것.
	: layer 출력값에 대한 normalize -> 이게 왜 도움이 될까

감마 & 베타
	: 감마는 1/감마 -> 시그마 앞에 가서 붙는데, 시그마 잘 설정하면 필요없지 않냐, 베타도 뮤/시그마
	: 굳이 저걸 곱해주고 더해주고를 왜 하냐? 
	: batch normalization 에서 추가로 저런 학습 파라미터를 적용해서 하는게 좋다.
	: batch 마다 다르게 비중을 조정하기 위해서, extra learnable parameter를 활용 (감마, 베타)
	: 어떤게 정답이다 는 없지만, 